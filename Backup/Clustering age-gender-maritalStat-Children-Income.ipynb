{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Mar 15 11:01:32 2022\n",
    "\n",
    "@author: Simone\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import DBSCAN,KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pyproj\n",
    "import json\n",
    "from shapely.geometry import Point, mapping\n",
    "from functools import partial\n",
    "from shapely.ops import transform\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "df = pd.read_csv(\"New/datasetItalyComplete.csv\", dtype = {\"ITTER107\":str})\n",
    "df = df.loc[df['Sesso'] != \"totale\"]\n",
    "df = df.loc[df['Stato civile'] != \"totale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['populPerKm2'] = df['Value']/df['Size km2']\n",
    "df = df.loc[df['populPerKm2'] > 10]#Filter out small villages\n",
    "df = df.reset_index().drop([\"index\"],axis =1)#.to_csv(\"datasetItalyCompleteNoSmallVill.csv\", encoding='utf-8',index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5bff9b",
   "metadata": {},
   "source": [
    "## Get only Lombardy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lombardy = pd.read_excel('New/Elenco-comuni-italiani.xls', sheet_name=0) #reads the first sheet of your excel file\n",
    "# lombardy = lombardy[lombardy['Denominazione Regione']=='Lombardia'] #Filtering dataframe\n",
    "# df[\"ITTER107\"] = pd.to_numeric(df[\"ITTER107\"])\n",
    "# df = pd.merge(df, lombardy,how='inner',left_on=['ITTER107'],right_on=['Codice Comune formato alfanumerico'])\n",
    "# df.drop(lombardy.columns,axis = 1, inplace = True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912b6b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)\n",
    "# heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b01615",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSliced = df[['lng','lat','Sesso','Stato civile','AgeGroup','AvgNumberOfComponents','Redditi 2020']]\n",
    "catColumnsPos = [dfSliced.columns.get_loc(col) for col in list(dfSliced.select_dtypes('object').columns)]\n",
    "catColumnsPos += [dfSliced.columns.get_loc(col) for col in list(dfSliced.select_dtypes('category').columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No if kprot\n",
    "dfSlicedNoCat = pd.get_dummies(dfSliced)\n",
    "dfSlicedNoCat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb64bf2",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d8c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# x = dfSliced[[\"AvgNumberOfComponents\",\"Redditi 2020\"]].values #returns a numpy array\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(x)\n",
    "# dfSliced[[\"AvgNumberOfComponents\",\"Redditi 2020\"]] = pd.DataFrame(\n",
    "#     x_scaled,columns=dfSliced[[\"AvgNumberOfComponents\",\"Redditi 2020\"]].columns,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c335248",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e844886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(dfSlicedNoCat[[\"AvgNumberOfComponents\",\"Redditi 2020\"]].values)\n",
    "scaled_features_df = pd.DataFrame(scaled_features, \n",
    "                                  index=dfSlicedNoCat[[\"AvgNumberOfComponents\",\"Redditi 2020\"]].index, \n",
    "                                  columns=dfSlicedNoCat[[\"AvgNumberOfComponents\",\"Redditi 2020\"]].columns)\n",
    "dfSlicedNoCat[[\"AvgNumberOfComponents\",\"Redditi 2020\"]] = scaled_features_df\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(dfSliced[[\"AvgNumberOfComponents\",\"Redditi 2020\"]].values)\n",
    "scaled_features_df = pd.DataFrame(scaled_features, \n",
    "                                  index=dfSliced[[\"AvgNumberOfComponents\",\"Redditi 2020\"]].index, \n",
    "                                  columns=dfSliced[[\"AvgNumberOfComponents\",\"Redditi 2020\"]].columns)\n",
    "dfSliced[[\"AvgNumberOfComponents\",\"Redditi 2020\"]] = scaled_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSlicedNoCat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98baa2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa38c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# heatmap = sns.heatmap(dfSlicedNoCat.corr(), vmin=-1, vmax=1, annot=True)\n",
    "# heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ff193",
   "metadata": {},
   "source": [
    "## Elbow KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ffc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose optimal K using Elbow method\n",
    "# distortions = []\n",
    "# inertias = []\n",
    "\n",
    "# rangeCl = range(10, 100)\n",
    "\n",
    "# print(catColumnsPos)\n",
    "# for cluster in rangeCl:\n",
    "#     kmeans = KMeans( n_clusters = cluster, init = 'k-means++', random_state = 0)\n",
    "#     categories = kmeans.fit_predict(dfSlicedNoCat)\n",
    "#     distortions.append(sum(np.min(cdist(dfSlicedNoCat, kmeans.cluster_centers_,\n",
    "#                                         'euclidean'), axis=1)) / dfSlicedNoCat.shape[0])\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "#     print('Cluster initiation: {}'.format(cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7635d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Data viz\n",
    "\n",
    "# plt.plot(rangeCl, inertias, 'bx-')\n",
    "# plt.xlabel('Values of K')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('The Elbow Method using Inertia')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(rangeCl, distortions, 'bx-')\n",
    "# plt.xlabel('Values of K')\n",
    "# plt.ylabel('Distortion')\n",
    "# plt.title('The Elbow Method using Distortion')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a937c96",
   "metadata": {},
   "source": [
    "### Clustering KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nClusters = 170 #88 Lombardy #170 Italy #250 too many low silh\n",
    "\n",
    "kmeans = KMeans( n_clusters = nClusters, init = 'k-means++', random_state = 0)\n",
    "kMeansCategories = kmeans.fit_predict(dfSlicedNoCat)\n",
    "df[\"cluster\"] = kMeansCategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df#.to_csv(\"clusteringItalia.csv\", encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecfcf44",
   "metadata": {},
   "source": [
    "## Elbow KPrototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1558ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Choose optimal K using Elbow method\n",
    "# cost = []\n",
    "\n",
    "\n",
    "# print(catColumnsPos)\n",
    "# for cluster in range(60, 86):\n",
    "#     try:\n",
    "#         kprototype = KPrototypes(n_jobs = -1, n_clusters = cluster, init = 'Huang', random_state = 0)\n",
    "#         kprototype.fit_predict(dfSliced, categorical = catColumnsPos)\n",
    "#         cost.append(kprototype.cost_)\n",
    "#         print('Cluster initiation: {}'.format(cluster))\n",
    "#     except:\n",
    "#         break\n",
    "# # Converting the results into a dataframe and plotting them\n",
    "# #Data viz\n",
    "\n",
    "# plt.plot( range(60, 86), cost, 'bx-')\n",
    "# plt.xlabel('Values of K')\n",
    "# plt.ylabel('Distortion')\n",
    "# plt.title('The Elbow Method using Distortion')\n",
    "# plt.show()\n",
    "# plt.savefig('elbow.png', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93679d",
   "metadata": {},
   "source": [
    "### Clustering KPrototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmodes.kprototypes import KPrototypes\n",
    "\n",
    "nClusters = 4 #40\n",
    "\n",
    "catColumnsPos = [dfSliced.columns.get_loc(col) for col in list(dfSliced.select_dtypes('object').columns)]\n",
    "catColumnsPos += [dfSliced.columns.get_loc(col) for col in list(dfSliced.select_dtypes('category').columns)]\n",
    "kprototype = KPrototypes(n_jobs = -1, n_clusters = nClusters, init = 'Huang', random_state = 0)\n",
    "KProtcategories = kprototype.fit_predict(dfSliced, categorical = catColumnsPos)\n",
    "df[\"cluster\"] = KProtcategories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748dd48",
   "metadata": {},
   "source": [
    "## SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e06f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hyperopt import fmin, hp, Trials, tpe, STATUS_OK\n",
    "# from minisom import MiniSom\n",
    "# # Setting some parameters in advance\n",
    "# x = y = 10\n",
    "# input_len = dfSlicedNoCat.shape[1]\n",
    "# sigma = 1.0\n",
    "# learning_rate = 0.5\n",
    "# iterations = 1000\n",
    "\n",
    "# space = {\n",
    "#     'sig': hp.uniform('sig',0.001,5.0),\n",
    "#     'learning_rate': hp.uniform('learning_rate',0.001,1),\n",
    "#     'x' :  hp.uniform('x',10,100),\n",
    "#     'y' :  hp.uniform('y',10,100)\n",
    "# }\n",
    "\n",
    "# def opt_map(space):\n",
    "#     sig = space['sig']\n",
    "#     learning_rate = space['learning_rate']\n",
    "#     x = space['x']\n",
    "#     y = space['y']\n",
    "#     val = MiniSom(x=int(x), y=int(y), input_len=input_len, sigma=sigma, learning_rate=learning_rate).quantization_error(dfSlicedNoCat.values)\n",
    "#     #print(\"Now,the quantization error is {}\\n\".format(val))\n",
    "#     return {'loss':val, 'status':STATUS_OK}\n",
    "\n",
    "# trials = Trials()\n",
    "# best_params = fmin(fn=opt_map,space=space,algo=tpe.suggest,max_evals=10,trials=trials)\n",
    "# print(\"The best sigma value after 50 iterations is {}\".format(best_params['sig']))\n",
    "# print(\"The best learning_rate after 50 iterations is {}\".format(best_params['learning_rate']))\n",
    "# print(\"The best x after 50 iterations is {}\".format(int(best_params['x'])))\n",
    "# print(\"The best y after 50 iterations is {}\".format(int(best_params['y'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ece6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hyperopt import fmin, hp, Trials, tpe, STATUS_OK\n",
    "# from minisom import MiniSom\n",
    "# # Setting some parameters in advance\n",
    "# x = y = 10\n",
    "# input_len = dfSlicedNoCat.shape[1]\n",
    "# sigma = 1.0\n",
    "# learning_rate = 0.5\n",
    "# iterations = 1000\n",
    "\n",
    "# space = {\n",
    "#     'sig': hp.uniform('sig',0.001,5.0),\n",
    "#     'learning_rate': hp.uniform('learning_rate',0.001,1),\n",
    "#     'x' :  hp.uniform('x',10,100),\n",
    "#     'y' :  hp.uniform('y',10,100)\n",
    "# }\n",
    "\n",
    "# def opt_map(space):\n",
    "#     sig = space['sig']\n",
    "#     learning_rate = space['learning_rate']\n",
    "#     x = space['x']\n",
    "#     y = space['y']\n",
    "#     som = MiniSom(x=int(x), y=int(y), input_len=input_len, sigma=sigma, \n",
    "#                   learning_rate=learning_rate)\n",
    "#     som.train_batch(dfSlicedNoCat.values, 10000)\n",
    "#     winner_coordinates = np.array([som.winner(x) for x in dfSlicedNoCat.values]).T\n",
    "#     # with np.ravel_multi_index we convert the bidimensional\n",
    "#     # coordinates to a monodimensional index\n",
    "#     SOMCategories = np.ravel_multi_index(winner_coordinates, (int(x),int(y)))\n",
    "#     val = -silhouette_score(dfSlicedNoCat,SOMCategories,sample_size = int(len(dfSlicedNoCat)/5),random_state = 0)\n",
    "#     #print(\"Now,the quantization error is {}\\n\".format(val))\n",
    "#     return {'loss':val, 'status':STATUS_OK}\n",
    "\n",
    "# trials = Trials()\n",
    "# best_params = fmin(fn=opt_map,space=space,algo=tpe.suggest,max_evals=10,trials=trials)\n",
    "# print(\"The best sigma value after 50 iterations is {}\".format(best_params['sig']))\n",
    "# print(\"The best learning_rate after 50 iterations is {}\".format(best_params['learning_rate']))\n",
    "# print(\"The best x after 50 iterations is {}\".format(int(best_params['x'])))\n",
    "# print(\"The best y after 50 iterations is {}\".format(int(best_params['y'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d5bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minisom import MiniSom\n",
    "\n",
    "# With quantization_error\n",
    "som_shape = (41,42)\n",
    "lr = 0.9310156988049898\n",
    "sig = 1.590577785080987\n",
    "\n",
    "# With topographic error\n",
    "# som_shape = (35,10)\n",
    "# lr = 0.19228430625877874\n",
    "# sig = 2.0404878138221725\n",
    "\n",
    "# With silhouette\n",
    "# som_shape = (61,33)\n",
    "# lr = 0.4207120861264718\n",
    "# sig = 4.443618124736826\n",
    "\n",
    "# With silhouette more iterations\n",
    "# som_shape = (14,91)\n",
    "# lr = 0.45328188042355616\n",
    "# sig = 2.0044976649797683\n",
    "\n",
    "som = MiniSom(x=som_shape[0], y=som_shape[1], input_len=dfSlicedNoCat.shape[1], learning_rate = lr, sigma = sig)\n",
    "som.train_batch(dfSlicedNoCat.values, 100000,  verbose=True)\n",
    "# each neuron represents a cluster\n",
    "winner_coordinates = np.array([som.winner(x) for x in dfSlicedNoCat.values]).T\n",
    "# with np.ravel_multi_index we convert the bidimensional\n",
    "# coordinates to a monodimensional index\n",
    "SOMCategories = np.ravel_multi_index(winner_coordinates, som_shape)\n",
    "df[\"cluster\"] = SOMCategories\n",
    "nClusters = max(SOMCategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d33e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import silhouette_score\n",
    "\n",
    "silhouette_score(dfSlicedNoCat,SOMCategories,sample_size = int(len(dfSlicedNoCat)/5),random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a96afc7",
   "metadata": {},
   "source": [
    "## Fuzzy C-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2506ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyclustering.cluster.fcm import fcm\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "from sklearn.metrics.cluster import silhouette_score\n",
    "\n",
    "# Choose optimal K using Elbow method\n",
    "silhouette = []\n",
    "\n",
    "rangeCl = range(10, 12)\n",
    "\n",
    "for cluster in rangeCl:\n",
    "    initial_centers = kmeans_plusplus_initializer(dfSlicedNoCat, cluster, kmeans_plusplus_initializer.FARTHEST_CENTER_CANDIDATE).initialize()\n",
    "    fcm_instance = fcm(dfSlicedNoCat.values, initial_centers)\n",
    "    fcm_instance.process()\n",
    "    clusters = fcm_instance.get_clusters()\n",
    "    df[\"cluster\"] = 1\n",
    "    i=0\n",
    "    for c in clusters:\n",
    "        df.iloc[c,df.columns.get_loc('cluster')] =i\n",
    "        i = i+1\n",
    "    FCMCategories = df[\"cluster\"].values\n",
    "    \n",
    "    silhouette.append(silhouette_score(dfSlicedNoCat,FCMCategories,\n",
    "                                               sample_size = int(len(dfSlicedNoCat)/5),random_state = 0))\n",
    "    print('Cluster initiation: {}'.format(cluster))\n",
    "#Data viz\n",
    "\n",
    "plt.plot(rangeCl, silhouette, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.title('The Elbow Method using Silhouette')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyclustering.cluster.fcm import fcm\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "\n",
    "amount_initial_centers = 20\n",
    "\n",
    "# initialize\n",
    "initial_centers = kmeans_plusplus_initializer(dfSlicedNoCat, amount_initial_centers, kmeans_plusplus_initializer.FARTHEST_CENTER_CANDIDATE).initialize()\n",
    " \n",
    "# create instance of Fuzzy C-Means algorithm\n",
    "fcm_instance = fcm(dfSlicedNoCat.values, initial_centers)\n",
    " \n",
    "# run cluster analysis and obtain results\n",
    "fcm_instance.process()\n",
    "clusters = fcm_instance.get_clusters()\n",
    "centers = fcm_instance.get_centers()\n",
    "\n",
    "print( len(centers))\n",
    "nClusters = len(clusters)\n",
    "df[\"cluster\"] = 1\n",
    "i=0\n",
    "for c in clusters:\n",
    "    df.iloc[c,df.columns.get_loc('cluster')] =i\n",
    "    i = i+1\n",
    "FCMCategories = df[\"cluster\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1960d2",
   "metadata": {},
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4549c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import OPTICS\n",
    "# #Minpts = domain knowledge\n",
    "# optics = OPTICS(min_samples=20, metric='euclidean' )\n",
    "# OPTICSCategories = optics.fit_predict(dfSlicedNoCat)\n",
    "# df[\"cluster\"] = OPTICSCategories\n",
    "# nClusters = max(OPTICSCategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4814134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(np.unique(OPTICSCategories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17289fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fefa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, hp, Trials, tpe, STATUS_OK\n",
    "import hdbscan\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "space = {'min_samples':  hp.uniform('min_samples',10,100),\n",
    "              'min_cluster_size': hp.uniform('min_cluster_size',100,600),  \n",
    "              'cluster_selection_method' : hp.choice('cluster_selection_method', ['eom','leaf']),\n",
    "              'metric' : hp.choice('metric', ['euclidean','manhattan']),\n",
    "             }\n",
    "\n",
    "def opt_map(space):\n",
    "    min_samples = space['min_samples']\n",
    "    min_cluster_size = space['min_cluster_size']\n",
    "    cluster_selection_method = space['cluster_selection_method']\n",
    "    metric = space['metric']\n",
    "    \n",
    "    hdb = hdbscan.HDBSCAN(min_samples=int(min_samples),min_cluster_size=int(min_cluster_size),\n",
    "                         cluster_selection_method=cluster_selection_method,metric=metric,\n",
    "                         algorithm=\"best\",gen_min_span_tree=True).fit(dfSlicedNoCat)\n",
    "    \n",
    "    val = -hdb.relative_validity_\n",
    "    return {'loss':val, 'status':STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=opt_map,space=space,algo=tpe.suggest,max_evals=4,trials=trials)\n",
    "print(\"The best min_samples value after 50 iterations is {}\".format(int(best_params['min_samples'])))\n",
    "print(\"The best min_cluster_size after 50 iterations is {}\".format(int(best_params['min_cluster_size'])))\n",
    "print(\"The best cluster_selection_method after 50 iterations is {}\".format(best_params['cluster_selection_method']))\n",
    "print(\"The best metric after 50 iterations is {}\".format(best_params['metric']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b5db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "hdbCategories = hdbscan.HDBSCAN(min_samples=22,min_cluster_size=102,\n",
    "                         cluster_selection_method='leaf',metric='euclidean',\n",
    "                         algorithm=\"best\",gen_min_span_tree=True).fit_predict(dfSlicedNoCat)\n",
    "hdbCategories\n",
    "df[\"cluster\"] = hdbCategories\n",
    "nClusters = max(hdbCategories)\n",
    "print(f\"DBCV score {hdbscan.validity.validity_index(dfSlicedNoCat.values,hdbCategories, metric = 'euclidean')}\")\n",
    "print(len(np.unique(hdbCategories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c51f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan.validity.validity_index(dfSlicedNoCat.values,hdbCategories, metric = 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2998157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 272 eom man -->0.219\n",
    "# 86 577 eom man --> 0.188\n",
    "# Provo ancora con relative, poi con validity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8929b0",
   "metadata": {},
   "source": [
    "## Xmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyclustering.cluster.xmeans import xmeans\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "\n",
    "# Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will\n",
    "# start analysis.\n",
    "amount_initial_centers = 10\n",
    "initial_centers = kmeans_plusplus_initializer(dfSlicedNoCat, amount_initial_centers).initialize()\n",
    "amount_max_centers = 3000\n",
    "\n",
    "xmeans_instance = xmeans(dfSlicedNoCat, initial_centers,amount_max_centers, tolerance = 0.002) #,tolerance = 0.001\n",
    "xmeans_instance.process()\n",
    "\n",
    "# Extract clustering results: clusters and their centers\n",
    "clusters = xmeans_instance.get_clusters() #cluster[0] are index of points attached to center 0\n",
    "centers = xmeans_instance.get_centers()\n",
    "\n",
    "print( len(centers))\n",
    "nClusters = len(clusters)\n",
    "df[\"cluster\"] = 1\n",
    "i=0\n",
    "for c in clusters:\n",
    "    df.iloc[c,df.columns.get_loc('cluster')] =i\n",
    "    i = i+1\n",
    "xmeansCategories = df[\"cluster\"].values\n",
    "#2380 with default tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842bd65",
   "metadata": {},
   "source": [
    "## Analyze clusters\n",
    "##### By changing cluster number and see which people comprises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in df.columns:\n",
    "#     if df.columns.get_loc(col) < 4:\n",
    "#         print(df.loc[df[\"cluster\"] == selectedCluster][col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcff09d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,nClusters):#For each cluster\n",
    "    print(\"Cluster #\" + str(i))\n",
    "    for col in df.columns: #For each \"important\" column\n",
    "        if (col not in [\"Value\",\"Territorio\",\"lat\",\"lng\",\"Size km2\",\"populPerKm2\",\"cluster\",\"ITTER107\"]):\n",
    "            print(df.loc[df[\"cluster\"] == i][col].astype(str).value_counts())\n",
    "    print()#Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec1519",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterNames = []\n",
    "for i in range(0,nClusters):#For each cluster\n",
    "    clusterName = \"\"\n",
    "    for col in df.columns: #For each \"important\" column\n",
    "        if (col not in [\"Value\",\"Territorio\",\"lat\",\"lng\",\"Size km2\",\"populPerKm2\",\"cluster\",\"ITTER107\"]):\n",
    "            if col == \"Redditi 2020\":\n",
    "                clusterName += \"Reddito medio \" + str(int(df.loc[df[\"cluster\"] == i][col].mean())) + \" \"\n",
    "            elif col == \"AvgNumberOfComponents\":\n",
    "                clusterName += \"Numero medio di componenti \" + str(round(df.loc[df[\"cluster\"] == i][col].mean(),2)) + \" \"\n",
    "            else:\n",
    "                clusterName += str(df.loc[df[\"cluster\"] == i][col].value_counts().idxmax()) + \" \"\n",
    "    clusterNames.append(clusterName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1154b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac00d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterNames = []\n",
    "for i in range(0,max(df.cluster)):#For each cluster\n",
    "    clusterName = \"\"\n",
    "    for col in df.columns: #For each \"important\" column\n",
    "        if (col not in [\"Value\",\"Territorio\",\"lat\",\"lng\",\"Size km2\",\"populPerKm2\",\"cluster\",\"ITTER107\"]):\n",
    "            if col == \"Redditi 2020\":\n",
    "                clusterName += str(int(df.loc[df[\"cluster\"] == i][col].mean())) + \" \"\n",
    "            elif col == \"AvgNumberOfComponents\":\n",
    "                clusterName += str(round(df.loc[df[\"cluster\"] == i][col].mean(),2)) + \" \"\n",
    "            else:\n",
    "                clusterName += str(df.loc[df[\"cluster\"] == i][col].value_counts().idxmax()) + \" \"\n",
    "    clusterName += str(i)\n",
    "    clusterNames.append(clusterName)\n",
    "\n",
    "splitted_search=[x.split(\" \") for x in clusterNames]\n",
    "clusterTypes = pd.DataFrame(splitted_search, columns =['Gender', 'MaritalStatus', 'AgeGroup',\n",
    "                                                            'AvgIncome','AvgNumComp','cluster'])\n",
    "clusterTypes[\"AvgIncome\"] = clusterTypes[\"AvgIncome\"].astype(int)\n",
    "clusterTypes[\"cluster\"] = clusterTypes[\"cluster\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c700a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr = clusterTypes.loc[(clusterTypes[\"AgeGroup\"] == ageGroup) & \n",
    "#                                         (clusterTypes[\"Gender\"] == gender) & \n",
    "#                                         (clusterTypes[\"MaritalStatus\"] == maritalStatus)]\n",
    "# income = 5 \n",
    "# # min(curr.values, key=lambda x:print(x))\n",
    "# def takeClosest(curr, targetIncome):\n",
    "#     closestIncome = curr.iloc[:1][\"AvgIncome\"].values[0]\n",
    "#     for i in range(1, curr.shape[0]):    \n",
    "#         currInc = curr.iloc[i:i+1][\"AvgIncome\"].values[0]\n",
    "#         if abs(currInc - targetIncome) < abs(closestIncome - targetIncome):\n",
    "#             closestIncome = currInc\n",
    "#     return closestIncome\n",
    "\n",
    "# df.loc[df[\"cluster\"] == curr.loc[curr[\"AvgIncome\"] == takeClosest(curr,income)][\"cluster\"].values[0]]\n",
    "# # curr.loc[curr[\"AvgIncome\"] == takeClosest(curr,income)][\"cluster\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e37e20",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aca969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract the data we're interested in\n",
    "# lat = df['lat'].values\n",
    "# lon = df['lng'].values\n",
    "# population = df['Value'].values\n",
    "# populationsqm = df['populPerKm2'].values\n",
    "# #area = cities['area_total_km2'].values\n",
    "selectedCluster = 84\n",
    "lat = df.loc[df[\"cluster\"] == selectedCluster]['lat'].values\n",
    "lon = df.loc[df[\"cluster\"] == selectedCluster]['lng'].values\n",
    "population = df.loc[df[\"cluster\"] == selectedCluster]['Value'].values\n",
    "populationsqm = df.loc[df[\"cluster\"] == selectedCluster]['populPerKm2'].values\n",
    "area = df.loc[df[\"cluster\"] == selectedCluster]['Size km2'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8dcc3c",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5066aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_point(point, points):\n",
    "    \"\"\" Find closest point from a list of points. \"\"\"\n",
    "    return points[cdist([point], points).argmin()]\n",
    "\n",
    "def getxyz(event):\n",
    "    s = ax.format_coord(event.xdata, event.ydata)\n",
    "    try:\n",
    "        out = [float(x.split('=')[1].strip()) for x in s.split(',')]\n",
    "    except:\n",
    "        out = None\n",
    "    return out\n",
    "\n",
    "def onclick(event): #Need to zoom  a bit for it to work (????)\n",
    "    out = getxyz(event)\n",
    "    if out != None:\n",
    "        az = ax.azim\n",
    "        el = ax.elev\n",
    "        if (abs(el) <= 2): #If elevation 0\n",
    "            if (abs(az) <= 2 or abs(abs(az) -180) <= 2): # get z and not x\n",
    "                zPt = out[2]\n",
    "                latPt = out[1]\n",
    "                closest = closest_point((zPt/100,latPt), list(zip(populationsqm/100, lat)))#Get closest point from picked\n",
    "                tx = '%s' % (df.loc[(df[\"populPerKm2\"] >= (closest[0]*100-0.1)) & \n",
    "                                    (df[\"populPerKm2\"] <= (closest[0]*100+0.1))&\n",
    "                                    (df[\"cluster\"] == selectedCluster)&\n",
    "                                    (df[\"lat\"] == closest[1])][\"Territorio\"].values[0])\n",
    "                ax.set_title(tx)\n",
    "            elif (abs(az)-90 <= 2):# get z and not y\n",
    "                zPt = out[2]\n",
    "                lonPt = out[0]\n",
    "                tx = out[0]\n",
    "                closest = closest_point((zPt/100,lonPt), list(zip(populationsqm/100, lon)))#Get closest point from picked\n",
    "                tx = '%s' % (df.loc[(df[\"populPerKm2\"] >= (closest[0]*100-0.1)) & \n",
    "                                    (df[\"populPerKm2\"] <= (closest[0]*100+0.1))&\n",
    "                                    (df[\"cluster\"] == selectedCluster)&\n",
    "                                    (df[\"lng\"] == closest[1])][\"Territorio\"].values[0])\n",
    "                ax.set_title(tx)\n",
    "            else:\n",
    "                ax.set_title(\"Please adjust azimuth\")\n",
    "        elif (abs(el-90) <= 2):\n",
    "            lonPt = out[0]#llcrnrlon+(urcrnrlon-llcrnrlon)/2 + event.xdata*((urcrnrlon-llcrnrlon)/2)/0.05481\n",
    "            latPt = out[1]#llcrnrlat+(urcrnrlat-llcrnrlat)/2 + event.ydata*((urcrnrlat-llcrnrlat)/2)/0.05481\n",
    "            closest = closest_point((lonPt,latPt), list(zip(lon, lat)))#Get closest point from picked\n",
    "            tx = '%s' % (df.loc[(df[\"lng\"] == closest[0])&\n",
    "                                (df[\"cluster\"] == selectedCluster)&\n",
    "                                (df[\"lat\"] == closest[1])][\"Territorio\"].values[0])\n",
    "            ax.set_title(tx)\n",
    "        else:\n",
    "            ax.set_title(\"Please adjust elevation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7492d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmRadius(c): #Define area of attractiveness\n",
    "    point = Point(c)\n",
    "    \n",
    "    local_azimuthal_projection = f\"+proj=aeqd +R=6371000 +units=m +lat_0={point.y} +lon_0={point.x}\"\n",
    "\n",
    "    wgs84_to_aeqd = partial(\n",
    "        pyproj.transform,\n",
    "        pyproj.Proj('+proj=longlat +datum=WGS84 +no_defs'),\n",
    "        pyproj.Proj(local_azimuthal_projection),\n",
    "    )\n",
    "\n",
    "    aeqd_to_wgs84 = partial(\n",
    "        pyproj.transform,\n",
    "        pyproj.Proj(local_azimuthal_projection),\n",
    "        pyproj.Proj('+proj=longlat +datum=WGS84 +no_defs'),\n",
    "    )\n",
    "\n",
    "    point_transformed = transform(wgs84_to_aeqd, point)\n",
    "\n",
    "    buffer = point_transformed.buffer(20_000)#Meters\n",
    "\n",
    "    buffer_wgs84 = transform(aeqd_to_wgs84, buffer)\n",
    "    dic = mapping(buffer_wgs84)\n",
    "    return np.asarray(dic['coordinates'])[0]\n",
    "\n",
    "##Mean based on: municipalities with more target population per sqmt influence more (can be **2 or **3)\n",
    "##Remove far away points from calculation\n",
    "def calculateWeightedMean(lat, lon, populationsqm):\n",
    "    avLat = np.average(a=lat[clf > 0], weights=populationsqm[clf > 0]**3)\n",
    "    avLon = np.average(a=lon[clf > 0], weights=populationsqm[clf > 0]**3)\n",
    "    return avLon,avLat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c850b2",
   "metadata": {},
   "source": [
    "## 3D map Lombardy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532803e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in range (0, 4):\n",
    "#     # Extract the data we're interested in\n",
    "#     lat = df.loc[df[\"cluster\"] == i]['lat'].values\n",
    "#     lon = df.loc[df[\"cluster\"] == i]['lng'].values\n",
    "#     population = df.loc[df[\"cluster\"] == i]['Value'].values\n",
    "#     populationsqm = df.loc[df[\"cluster\"] == i]['populPerKm2'].values\n",
    "#     area = df.loc[df[\"cluster\"] == i]['Size km2'].values\n",
    "fig = plt.figure(0)\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "# print(clusterNames[selectedCluster])\n",
    "\n",
    "llcrnrlat = 35.5#44\n",
    "urcrnrlat = 47\n",
    "llcrnrlon = 7#7\n",
    "urcrnrlon = 18.5#12\n",
    "# Create a basemap instance that draws the Earth layer\n",
    "bm = Basemap(llcrnrlat=llcrnrlat, llcrnrlon=llcrnrlon, \n",
    "            urcrnrlat=urcrnrlat, urcrnrlon=urcrnrlon,\n",
    "            projection='cyl', resolution='l', fix_aspect=False, ax=ax)\n",
    "# Add Basemap to the figure\n",
    "ax.add_collection3d(bm.drawcoastlines(linewidth=0.25))\n",
    "ax.add_collection3d(bm.drawcountries(linewidth=0.35))\n",
    "ax.view_init(azim=-90, elev=90)\n",
    "ax.set_xlabel('Longitude (°E)', labelpad=20)\n",
    "ax.set_ylabel('Latitude (°N)', labelpad=20)\n",
    "ax.set_zlabel('Population per sqkm', labelpad=20)\n",
    "ax.set_zlim(0., 400)\n",
    "\n",
    "colormap = np.arange(0,15)\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "clf = IsolationForest( random_state=0).fit_predict(np.array((lon,lat)).T)#Schema a blochi pptx\n",
    "print(np.array((lon,lat)).T)\n",
    "\n",
    "lonC, latC = calculateWeightedMean(lat, lon, populationsqm)\n",
    "p = ax.scatter(lon, lat, (populationsqm), picker = True, s = area,c=colormap[clf])#Points # c=colormap[categories]\n",
    "centers = ax.scatter(xs=lonC, ys=latC, edgecolor='black', c='white', marker = '*')#Centers\n",
    "\n",
    "circle = kmRadius((lonC,latC))\n",
    "r = ax.scatter(xs=circle[:,0], ys=circle[:,1], c='black', marker = '1')#Radius\n",
    "\n",
    "closest = closest_point((lonC,latC), list(zip(lon, lat)))#Get closest point from picked\n",
    "\n",
    "tx = 'Center is located in: %s' % (df.loc[(df[\"lng\"] == closest[0])&\n",
    "                                          (df[\"cluster\"] == selectedCluster)&\n",
    "                                          (df[\"lat\"] == closest[1])][\"Territorio\"].values[0])\n",
    "ax.set_title(tx)\n",
    "print(tx)\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)#Get mouse click\n",
    "\n",
    "fig.show\n",
    "print(clusterNames[selectedCluster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a16feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df[\"cluster\"] == selectedCluster) & (df[\"Territorio\"] == \"Cusano Milanino\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[(df[\"cluster\"] == selectedCluster) & (df[\"Territorio\"] == \"Lecco\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578a45f",
   "metadata": {},
   "source": [
    "## PCA no categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a082cf7",
   "metadata": {},
   "source": [
    "### Graph num of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ed0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "pca = PCA(n_components=21, random_state = 0)\n",
    "principalComponents = pca.fit_transform(dfSlicedNoCat)\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "print(\"Variance remained\", np.sum(pca.explained_variance_ratio_))\n",
    "#% of info contained with n components\n",
    "plt.figure(figsize = (8,8))\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Explained variance\")\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(\"Elbow_PCA.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac4207",
   "metadata": {},
   "source": [
    "### 2 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6fb921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, random_state = 0)\n",
    "principalComponents = pca.fit_transform(dfSlicedNoCat)\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "print(\"Info remained with 2 components:\", np.sum(pca.explained_variance_ratio_))\n",
    "#% of info contained with n components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece7e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "sns.scatterplot(x = principalComponents[:,0], y = principalComponents[:,1]) \n",
    "plt.xlabel('Principal Component 1', fontsize = 15)\n",
    "plt.ylabel('Principal Component 2', fontsize = 15)\n",
    "plt.title('2 component PCA', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879be7f2",
   "metadata": {},
   "source": [
    "### 3 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d073bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3, random_state = 0)\n",
    "principalComponents = pca.fit_transform(dfSlicedNoCat)\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "print(\"Info remained with 3 components:\", np.sum(pca.explained_variance_ratio_))\n",
    "#% of info contained with n components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb46c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xax = principalComponents[:,0]\n",
    "Yax = principalComponents[:,1]\n",
    "Zax = principalComponents[:,2]\n",
    "\n",
    "cdict = {0:'red',1:'green'}\n",
    "labl = {0:'Malignant',1:'Benign'}\n",
    "marker = {0:'*',1:'o'}\n",
    "alpha = {0:.3, 1:.5}\n",
    "\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.scatter(Xax, Yax, Zax)\n",
    "# for loop ends\n",
    "ax.set_xlabel(\"First Principal Component\", fontsize=14)\n",
    "ax.set_ylabel(\"Second Principal Component\", fontsize=14)\n",
    "ax.set_zlabel(\"Third Principal Component\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e91edee",
   "metadata": {},
   "source": [
    "## FAMD (PCA  + categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e80e5",
   "metadata": {},
   "source": [
    "### Graph num of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56689738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prince\n",
    "\n",
    "famd = prince.FAMD(\n",
    "     n_components=18,\n",
    "     n_iter = 3,\n",
    "     engine='auto',\n",
    "     random_state=0\n",
    ")\n",
    "famd = famd.fit(dfSliced)\n",
    "famdTr = famd.transform(dfSliced)\n",
    "print(\"Variance remained\",np.sum(famd.explained_inertia_))\n",
    "plt.figure(figsize = (8,8))\n",
    "\n",
    "plt.plot(np.cumsum(famd.explained_inertia_))\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Explained variance\")\n",
    "plt.grid()\n",
    "\n",
    "# plt.savefig(\"Elbow_FAMD.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa049b97",
   "metadata": {},
   "source": [
    "### 2 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b921a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "famd = prince.FAMD(\n",
    "     n_components=2,\n",
    "     n_iter = 3,\n",
    "     engine='auto',\n",
    "     random_state=0\n",
    ")\n",
    "famd = famd.fit(dfSliced)\n",
    "famdTr = famd.transform(dfSliced)\n",
    "print(\"Variance remained\",np.sum(famd.explained_inertia_))\n",
    "\n",
    "ax = famd.plot_row_coordinates(\n",
    "     X = dfSliced,\n",
    "     ax=None,\n",
    "     figsize=(6, 6),\n",
    "     x_component=0,\n",
    "     y_component=1,\n",
    "     ellipse_outline=True,\n",
    "     ellipse_fill=True,\n",
    "     show_points=True\n",
    ")\n",
    "# ax.get_figure().savefig('famd_row_coordinates.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0e9de1",
   "metadata": {},
   "source": [
    "## Compare clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098423ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHScore = []# Higher better\n",
    "# DBScore = []# 0 best\n",
    "score = []# 1 best, -1 worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b04bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.cluster import homogeneity_score\n",
    "# from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "# from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics.cluster import calinski_harabasz_score\n",
    "from sklearn.metrics.cluster import davies_bouldin_score\n",
    "from sklearn.metrics.cluster import silhouette_score\n",
    "\n",
    "# homogeneity_score(kMeansCategories,Y)\n",
    "# adjusted_mutual_info_score(kMeansCategories, [0, 0, 1, 1])\n",
    "# adjusted_rand_score(kMeansCategories, [0, 0, 1, 1])\n",
    "# calinski_harabasz_score (dfSlicedNoCat,kMeansCategories)\n",
    "# davies_bouldin_score(dfSlicedNoCat,kMeansCategories)\n",
    "# silhouette_score(dfSlicedNoCat,kMeansCategories,sample_size = int(len(dfSlicedNoCat)/5),random_state = 0) \n",
    "\n",
    "# score.append([\"KMeans with Elbow\",\n",
    "#               calinski_harabasz_score (dfSlicedNoCat,kMeansCategories),\n",
    "#               davies_bouldin_score(dfSlicedNoCat,kMeansCategories),\n",
    "#               silhouette_score(dfSlicedNoCat,kMeansCategories,sample_size = int(len(dfSlicedNoCat)/5),random_state = 0),\n",
    "#               len(np.unique(kMeansCategories))])\n",
    "\n",
    "# score.append([\"KPrototype with Elbow\",\n",
    "#                 calinski_harabasz_score (dfSlicedNoCat,KProtcategories),\n",
    "#                 davies_bouldin_score(dfSlicedNoCat,KProtcategories),\n",
    "#               silhouette_score(dfSlicedNoCat,KProtcategories,sample_size = int(len(dfSliced)/5),random_state = 0),\n",
    "#             len(np.unique(KProtcategories))])\n",
    "\n",
    "# score.append([\"Xmeans\",calinski_harabasz_score (dfSlicedNoCat,xmeansCategories),\n",
    "#               davies_bouldin_score(dfSlicedNoCat,xmeansCategories),\n",
    "#               silhouette_score(dfSlicedNoCat,xmeansCategories,sample_size = int(len(dfSlicedNoCat)/5),random_state = 0),\n",
    "#              len(np.unique(xmeansCategories))])\n",
    "\n",
    "# score.append([\"SOM\",calinski_harabasz_score (dfSlicedNoCat,SOMCategories),\n",
    "#               davies_bouldin_score(dfSlicedNoCat,SOMCategories),\n",
    "#               silhouette_score(dfSlicedNoCat,SOMCategories,sample_size = int(len(dfSlicedNoCat)/5),random_state = 0),\n",
    "#             len(np.unique(SOMCategories))])\n",
    "\n",
    "# score.append([\"HDBSCAN\",calinski_harabasz_score (dfSlicedNoCat,hdbCategories),\n",
    "#               davies_bouldin_score(dfSlicedNoCat,hdbCategories),\n",
    "#               hdbscan.validity.validity_index(dfSlicedNoCat.values,hdbCategories, metric = 'euclidean'),\n",
    "#              len(np.unique(hdbCategories))])\n",
    "\n",
    "score.append([\"FCM\",calinski_harabasz_score (dfSlicedNoCat,FCMCategories),\n",
    "              davies_bouldin_score(dfSlicedNoCat,FCMCategories),\n",
    "              silhouette_score(dfSlicedNoCat,FCMCategories,sample_size = int(len(dfSlicedNoCat)/5),random_state = 0),\n",
    "             len(np.unique(FCMCategories))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba48d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(score, columns = ['Algorithm', 'CH Value(higher better)','DB Value(0 best)', 'Silhouette value(1 best)', 'Number of clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0db986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
